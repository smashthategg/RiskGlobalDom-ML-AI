{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9908a38f",
   "metadata": {},
   "source": [
    "# Benchmarking RISK Combat and Probability Engines\n",
    "\n",
    "This notebook contains benchmark tests and performance comparisons for different implementations of combat simulation and probability estimation in our Python version of the RISK game.\n",
    "\n",
    "### Goals\n",
    "1. Evaluate runtime efficiency of various `estimate_win_probability` implementations\n",
    "2. Compare single-battle simulation methods (`battle` functions) for speed\n",
    "3.  Explore vectorized vs iterative vs parallel approaches\n",
    "4.  Document observations and trade-offs\n",
    "\n",
    "### Structure\n",
    "1. Setup: imports, parameters, utility functions\n",
    "2. Baseline implementations benchmarking\n",
    "3. Vectorized implementations benchmarking\n",
    "4. Parallel and compiled (Numba) implementations benchmarking\n",
    "5. Summary and conclusions\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup\n",
    "\n",
    "This section prepares the environment for benchmarking the RISK combat and probability engines.\n",
    "\n",
    "### Tested Methods\n",
    "\n",
    "- **`battle(num_attackers, num_defenders)`**:  \n",
    "  Simulates a single RISK battle from start to finish between the attacker and defender armies. It repeatedly simulates battle rounds until one side runs out of troops, returning the remaining armies for both sides.\n",
    "\n",
    "- **`estimate_win_probability(num_attackers, num_defenders, reps)`**:  \n",
    "  Uses Monte Carlo simulation by running `reps` number of full battles to estimate the probability that the attacker wins (i.e., the defender is eliminated). This method aggregates results over many simulations to provide a statistical estimate of the attacker's success chance.\n",
    "\n",
    "### Imports and Parameters\n",
    "\n",
    "We will import required libraries and define default parameters such as the number of troops and simulation repetitions to ensure consistency in benchmarking:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "47d34db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, random\n",
    "import numpy as np\n",
    "\n",
    "# Past a certain troop limit, simulations get extremely slow and we would have to rely on complicated formulas/approximations.\n",
    "num_attackers = 70\n",
    "num_defenders = 70\n",
    "\n",
    "reps = 10000\n",
    "num_trials = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae0e27b",
   "metadata": {},
   "source": [
    "### Key points for choosing number of repetitions `reps`:\n",
    "\n",
    "**1. Monte Carlo estimate variance:**\n",
    "\n",
    "When estimating a probability $p$ via $n$ independent trials, the estimate $\\hat{p}$ is a random variable with approximately a Binomial distribution:\n",
    "\n",
    "$$\n",
    "\\hat{p} \\sim \\text{Binomial}(n,p) / n\n",
    "$$\n",
    "\n",
    "So the **standard error** of $\\hat{p}$ is:\n",
    "\n",
    "$$\n",
    "\\text{SE} = \\sqrt{\\frac{p(1-p)}{n}}\n",
    "$$\n",
    "\n",
    "Using this, we can build a confidence interval around $\\hat{p}$: \n",
    "\n",
    "$$\n",
    "\\hat{p} \\pm z_{\\alpha/2} \\times \\text{SE}\n",
    "$$\n",
    "\n",
    "**2. Choosing $n$ for desired margin of error:**\n",
    "\n",
    "If you want to estimate $\\hat{p}$ to be within a margin of error $E$, rearrange:\n",
    "\n",
    "$$\n",
    "E = z_{\\alpha/2} \\times \\sqrt{\\frac{p(1-p)}{n}} \\implies n = \\frac{z^2_{\\alpha/2}p(1-p)}{E^2}\n",
    "$$\n",
    "\n",
    "Suppose we want to estimate $p$ with a margin of error of 0.01 with a 95% confidence interval. Assuming the worst case, $p = 0.5$ (which maximizes variance $p(1-p)$):\n",
    "\n",
    "$$\n",
    "n = \\frac{z^2_{0.05/2} \\times 0.5 \\times (1-0.5)}{0.01^2} \n",
    "\\approx \\frac{(1.96)^2 \\times 0.25}{0.0001} \n",
    "= \\boxed{9604}\n",
    "$$\n",
    "\n",
    "**3. Understanding the precision vs. speed problem**\n",
    "\n",
    "We would need about $n = 10000$ repetitions to get what I would consider a barely passing estimate of the probability. In the actual RISK game, estimates are rounded to the nearest percent (0.01), so using a margin of error of 0.5% (0.005) would make more sense ($n \\approx 38416$). Unfortunately, the former is probably the best we can do, since over-tightening the margin of error comes at the cost of square scaling repetitions and thus unacceptably long computational times. The pattern is shown in the table below:\n",
    "\n",
    "| Margin of Error ($E$) | Upper bound $n$ for 95% CI |\n",
    "| ------------------- | -------------------------- |\n",
    "| 0.1% (0.001)        | \\~1,000,000                |\n",
    "| 0.5% (0.005)        | \\~40,000                   |\n",
    "| **1% (0.01)**       | **\\~10,000**               |\n",
    "| 2% (0.02)           | \\~2,400                    |\n",
    "| 5% (0.05)           | \\~400                      |\n",
    "| 10% (0.10)          | \\~100                      |\n",
    "\n",
    "**Final answer? Use `reps = 10000` for Monte Carlo estimation.**\n",
    "\n",
    "### Utility functions\n",
    "\n",
    "I've created a **generic benchmark wrapper function** that accepts any callable (function) along with its arguments, runs it a specified number of trials, and returns summary statistics like total time, average time per call, and standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "966967ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_test(func, *args, trials=num_trials, verbose=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Benchmark wrapper to time execution of a function over multiple runs.\n",
    "\n",
    "    Args:   func (callable): Function to benchmark.\n",
    "            *args: Positional arguments to pass to func.\n",
    "            trials (int): Number of trials to run func.\n",
    "            verbose (bool): Whether to print summary stats.\n",
    "            **kwargs: Keyword arguments to pass to func.\n",
    "\n",
    "    Returns:    dict: Summary statistics with keys:\n",
    "\n",
    "            - total_time (float): Total time for all trials (seconds).\n",
    "            - avg_time (float): Average time per call (seconds).\n",
    "            - std_time (float): Standard deviation of times (seconds).\n",
    "            - times (np.ndarray): Array of individual run times.\n",
    "            - result: The result of the last function call.\n",
    "    \"\"\"\n",
    "    times = []\n",
    "    result = None\n",
    "    for _ in range(trials):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        times.append(end - start)\n",
    "    times = np.array(times)\n",
    "    summary = {\n",
    "        \"total_time\": times.sum(),\n",
    "        \"avg_time\": times.mean(),\n",
    "        \"std_time\": times.std(),\n",
    "        \"times\": times,\n",
    "        \"result\": result\n",
    "    }\n",
    "    if verbose:\n",
    "        print(f\"Function '{func.__name__}' benchmarked over {trials} trials:\")\n",
    "        print(f\"  Total time: {summary['total_time']:.4f} sec\")\n",
    "        print(f\"  Average time per call: {summary['avg_time']:.6f} sec\")\n",
    "        print(f\"  Std dev of times: {summary['std_time']:.6f} sec\")\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9f6750",
   "metadata": {},
   "source": [
    "## 2. Baseline implementations benchmarking\n",
    "\n",
    "We define our baseline `battle` and `estimate_win_probability` functions as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a6f8068d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def battle(a=num_attackers, d=num_defenders):\n",
    "        \"\"\"\n",
    "        Simulates a full battle in according with RISK True Random settings.\n",
    "\n",
    "        Args:\n",
    "            a (int): Number of attackers\n",
    "            d (int): Number of defenders\n",
    "                \n",
    "        Returns:    \n",
    "            (int, int): The total remaining troops on both sides.\n",
    "        \"\"\"\n",
    "        while a > 0 and d > 0:\n",
    "            # Attacker rolls 3 dice (while they have 3+ troops)\n",
    "            atk_dice = min(3, a) \n",
    "            # Defender rolls 2 dice (while they have 2+ troops)\n",
    "            def_dice = min(2, d)\n",
    "\n",
    "            att_rolls = np.random.randint(1, 7, size=atk_dice)\n",
    "            def_rolls = np.random.randint(1, 7, size=def_dice)\n",
    "\n",
    "            # Sort rolls by descending\n",
    "            att_top = np.sort(att_rolls)[::-1]\n",
    "            def_top = np.sort(def_rolls)[::-1]\n",
    "\n",
    "            # Compare best dice pairs, lesser side loses one troop.\n",
    "            for i in range(min(atk_dice, def_dice)):\n",
    "                # If dice are a tie, defender wins.\n",
    "                if att_top[i] > def_top[i]: \n",
    "                    d -= 1\n",
    "                else:\n",
    "                    a -= 1\n",
    "\n",
    "        return a, d\n",
    "        \n",
    "def estimate_win_probability(a=num_attackers, d=num_defenders, n=reps):\n",
    "    \"\"\"\n",
    "    Estimates the likelihood of winning an attack via Monte Carlo.\n",
    "\n",
    "    Args:\n",
    "        a (int): Number of attackers\n",
    "        d (int): Number of defenders\n",
    "        n (int): Number of repetitions\n",
    "            \n",
    "    Returns: \n",
    "        float: The percentage value estimated probability of winning.\n",
    "    \"\"\"\n",
    "    wins = 0\n",
    "    for _ in range(n):\n",
    "        _, defenders = battle(a, d)\n",
    "        if defenders == 0:\n",
    "            wins += 1\n",
    "    return round(wins / n * 100, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b489d689",
   "metadata": {},
   "source": [
    "These implementations are the simplest, so we would hope improvements can be made. Let's take a look at their benchmark times below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5953c727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'battle' benchmarked over 5 trials:\n",
      "  Total time: 0.0015 sec\n",
      "  Average time per call: 0.000301 sec\n",
      "  Std dev of times: 0.000602 sec\n",
      "Function 'estimate_win_probability' benchmarked over 5 trials:\n",
      "  Total time: 6.5049 sec\n",
      "  Average time per call: 1.300974 sec\n",
      "  Std dev of times: 0.029198 sec\n",
      "Function 'battle' benchmarked over 5 trials:\n",
      "  Total time: 0.0059 sec\n",
      "  Average time per call: 0.001185 sec\n",
      "  Std dev of times: 0.000229 sec\n",
      "Function 'estimate_win_probability' benchmarked over 5 trials:\n",
      "  Total time: 85.4915 sec\n",
      "  Average time per call: 17.098297 sec\n",
      "  Std dev of times: 4.331547 sec\n",
      "Function 'battle' benchmarked over 5 trials:\n",
      "  Total time: 2.7144 sec\n",
      "  Average time per call: 0.542887 sec\n",
      "  Std dev of times: 0.028830 sec\n"
     ]
    }
   ],
   "source": [
    "random.seed(\"smashthategg\")\n",
    "\n",
    "# Smaller army counts (faster)\n",
    "base_battle_10_summary = benchmark_test(battle, a=10, d=10) \n",
    "base_ewp_10_summary = benchmark_test(estimate_win_probability, a=10, d=10)\n",
    "\n",
    "# Default 70 army counts (slower)\n",
    "base_battle_summary = benchmark_test(battle) \n",
    "base_ewp_summary = benchmark_test(estimate_win_probability)\n",
    "\n",
    "# Huge battle, estimation impossible\n",
    "base_battle_15k_summary = benchmark_test(battle, a=15000, d=15000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6373ba4",
   "metadata": {},
   "source": [
    "Even though `random.seed` is set for reproducibility, you **will** get different results in runtime speed due to differences in hardware. At the very least, our hardwares should remain consistent throughout benchmark tests so the relative comparisons should still hold weight! My data and observations are as follows:\n",
    "\n",
    "- Calls to `battle` are extremely fast, as expected for a single-simulation function. However, since our main focus `estimate_win_probability` makes repeated calls to `battle`, optimizations to this function will still be of great benefit to us.\n",
    "- `estimate_win_probability`, even at low troop counts (`10` on each side), takes about **1 second** to calculate. At higher counts (`70`), it takes **10 times as long**. This is extremely slow if we are trying to utilize this function to recreate the *Balanced Blitz* setting, and **strictly impossible** to train reinforcement learning bots on. \n",
    "- We desperately have to find better implementations, or otherwise greatly increase the margin of error to allow for reduced repetitions at the cost of precision.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
